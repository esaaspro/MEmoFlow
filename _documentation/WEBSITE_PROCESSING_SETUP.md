# ğŸŒ Website Processing - Setup Guide

## ğŸ“¦ Dependencies

### Installed Packages
```json
{
  "jsdom": "^24.0.0",
  "@mozilla/readability": "^0.4.4",
  "@types/jsdom": "^21.1.6"
}
```

**Usage:**
```typescript
import { JSDOM } from "jsdom";
import { Readability } from "@mozilla/readability";
```

---

## ğŸ”„ Complete Flow

```
1. User clicks "Lien Site Web"
   â†“
2. Modal opens with URL input field
   â†“
3. User pastes website URL
   â†“
4. User clicks "GÃ©nÃ©rer la fiche"
   â†“
5. API validates URL format
   â†“
6. Fetch HTML with User-Agent header
   â†“
7. Parse HTML with JSDOM
   â†“
8. Extract main content with Readability
   â†“
9. Validate content length (> 500 chars)
   â†“
10. Generate summary with Groq AI
   â†“
11. Save note to database:
    - title: Extracted article title
    - type: "website"
    - summary_markdown: AI-generated content
   â†“
12. Redirect to /dashboard/notes/[noteId]
```

---

## ğŸ¯ Supported Websites

### Works Best With
- âœ… **Blog posts** (Medium, Dev.to, personal blogs)
- âœ… **News articles** (Le Monde, Le Figaro, etc.)
- âœ… **Wikipedia articles**
- âœ… **Documentation pages** (MDN, etc.)
- âœ… **Academic papers** (if HTML accessible)

### May Have Issues
- âš ï¸ **Paywalled content** (requires subscription)
- âš ï¸ **JavaScript-heavy sites** (SPA, React apps)
- âš ï¸ **Protected/Private pages** (login required)
- âš ï¸ **Very short pages** (< 500 characters)

---

## ğŸ”§ Technical Details

### HTML Fetching
```typescript
const response = await fetch(url, {
  headers: {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)...",
    Accept: "text/html,application/xhtml+xml...",
    "Accept-Language": "fr-FR,fr;q=0.9...",
  },
});
```

**Why User-Agent?**
- Some sites block requests without proper headers
- Mimics a real browser to avoid bot detection
- Improves compatibility with most websites

### Content Extraction
```typescript
const dom = new JSDOM(html, { url });
const reader = new Readability(dom.window.document);
const article = reader.parse();

// Returns:
{
  title: "Article Title",
  textContent: "Clean text without ads/navbars",
  excerpt: "Short summary",
  // ... other metadata
}
```

**What Readability Does:**
- âœ… Removes navigation bars
- âœ… Removes ads and sidebars
- âœ… Extracts main article content
- âœ… Preserves text structure
- âœ… Removes scripts and styles

---

## ğŸ“Š Content Validation

### Minimum Length
```typescript
if (textContent.length < 500) {
  throw new Error("Contenu insuffisant ou illisible");
}
```

**Why 500 characters?**
- Ensures meaningful content
- Filters out landing pages
- Better AI generation results

### Validation Checks
1. âœ… URL format valid (http/https)
2. âœ… HTML fetched successfully
3. âœ… Content extracted by Readability
4. âœ… Text content > 500 characters
5. âœ… Title extracted (or fallback)

---

## ğŸ§  AI Processing

### Input
- Clean text content (no HTML, no ads)
- Extracted by Readability

### Processing
- Uses existing `generateStudyContent()` function
- Groq AI with Llama 3.3 70B
- Generates structured Markdown summary

### Output Format
```markdown
# ğŸ“Š [Article Title]

## ğŸ¯ Points ClÃ©s
- Point 1
- Point 2

## ğŸ’¡ Astuce
[AI-generated tip]
```

---

## ğŸ“Š Database Structure

### Note Created
```typescript
{
  id: "uuid-generated",
  user_id: "auth-user-id",
  title: "Extracted Article Title",
  type: "website",
  summary_markdown: "# ğŸ“Š Article Title...",
  flashcards: [],
  created_at: "2026-01-13T14:32:00Z",
  updated_at: "2026-01-13T14:32:00Z"
}
```

**Note:** To store the original URL, you can:
1. Add a `metadata` JSONB column to `notes` table
2. Store URL in `metadata.original_url`
3. Or add a dedicated `original_url` column

---

## âš ï¸ Common Issues

### Issue 1: "Site protÃ©gÃ© ou inaccessible"
**Causes:**
- Site requires authentication
- Site blocks automated requests
- Site is down or unreachable
- CORS restrictions

**Solutions:**
- Try a different URL
- Check if site requires login
- Verify URL is correct
- Some sites may need different headers

### Issue 2: "Contenu insuffisant ou illisible"
**Causes:**
- Page is mostly JavaScript (SPA)
- Page is a landing page (not an article)
- Readability couldn't extract main content

**Solutions:**
- Use a page with clear article content
- Try a different article from the same site
- Some sites may not be compatible

### Issue 3: "Impossible d'extraire le contenu principal"
**Causes:**
- HTML structure is non-standard
- Page has no clear article content
- Readability parser failed

**Solutions:**
- Try a different article
- Some sites may need custom parsing

---

## ğŸ§ª Testing

### 1. Test URL Validation
```typescript
// Valid URLs
"https://fr.wikipedia.org/wiki/..."
"https://medium.com/@user/article"
"https://example.com/blog/post"

// Invalid URLs
"not-a-url"
"ftp://example.com"
"javascript:void(0)"
```

### 2. Test Content Extraction
```bash
npm run dev
# Visit http://localhost:3000/dashboard
# Click "Lien Site Web"
# Paste a blog/article URL
# Click "GÃ©nÃ©rer la fiche"
```

### 3. Test Flow
1. Select a blog post or article
2. Copy the URL
3. Paste in modal
4. Click "GÃ©nÃ©rer la fiche"
5. Wait for processing (~10-30 seconds)
6. Redirected to new note with AI-generated summary

### 4. Test Error Cases
- Invalid URL â†’ Validation error
- Protected site â†’ "Site protÃ©gÃ© ou inaccessible"
- Short page â†’ "Contenu insuffisant"
- JavaScript-heavy site â†’ May fail extraction

---

## ğŸš€ Production Optimization

### Error Handling
- âœ… URL format validation
- âœ… Fetch error handling
- âœ… Readability extraction error handling
- âœ… Content length validation
- âœ… AI generation error handling
- âœ… Database save error handling

### Performance
- **HTML Fetch:** ~1-3 seconds
- **Content Extraction:** ~0.5-1 second
- **AI Generation:** ~5-15 seconds
- **Total:** ~10-30 seconds (depending on content length)

### Limitations
- **JavaScript Sites:** May not work (SPA, React apps)
- **Paywalled Content:** Cannot access without subscription
- **Dynamic Content:** May miss content loaded via JS
- **Rate Limiting:** Some sites may rate-limit requests

---

## ğŸ¯ Future Enhancements

### 1. Store Original URL
```sql
-- Add metadata column
ALTER TABLE notes ADD COLUMN metadata JSONB DEFAULT '{}'::jsonb;

-- Store URL
{
  "original_url": "https://example.com/article",
  "extracted_title": "Article Title",
  "content_length": 5000
}
```

### 2. Custom Parsers
```typescript
// Site-specific parsers for better extraction
const parsers = {
  "wikipedia.org": parseWikipedia,
  "medium.com": parseMedium,
  // ...
};
```

### 3. Caching
```typescript
// Cache extracted content to avoid re-fetching
const cacheKey = `website:${url}`;
const cached = await redis.get(cacheKey);
```

### 4. Screenshot/Preview
```typescript
// Generate preview image of the page
const screenshot = await takeScreenshot(url);
```

### 5. Language Detection
```typescript
// Auto-detect language and adjust AI prompt
const language = detectLanguage(textContent);
```

---

## ğŸ” Security Considerations

### User-Agent Spoofing
- âœ… Used to improve compatibility
- âš ï¸ Some sites may detect and block
- âœ… Not malicious (just for content extraction)

### Rate Limiting
- âš ï¸ Respect robots.txt (future enhancement)
- âš ï¸ Implement rate limiting per user
- âš ï¸ Cache results to reduce requests

### Content Validation
- âœ… Validate URL format
- âœ… Sanitize extracted content
- âœ… Check content length
- âœ… Validate before AI processing

---

## âœ… Checklist

- [x] `jsdom` and `@mozilla/readability` installed
- [x] `@types/jsdom` installed
- [x] API route `/api/process-website` created
- [x] URL validation implemented
- [x] HTML fetching with User-Agent
- [x] Content extraction with Readability
- [x] Content length validation
- [x] AI generation integration
- [x] Database save logic
- [x] UI modal updated for website URLs
- [x] Loading states implemented
- [x] Error messages displayed
- [x] Redirect to note page after success

---

## ğŸ“ Example Usage

### Frontend (Modal)
```typescript
// User pastes URL
const url = "https://fr.wikipedia.org/wiki/Intelligence_artificielle";

// API call
const response = await fetch("/api/process-website", {
  method: "POST",
  headers: { "Content-Type": "application/json" },
  body: JSON.stringify({ url, userId }),
});

const { noteId } = await response.json();
router.push(`/dashboard/notes/${noteId}`);
```

### Backend (API)
```typescript
// Fetch HTML
const html = await fetchHtml(url);

// Extract content
const { title, textContent } = extractContent(html, url);

// Generate summary
const summary = await generateStudyContent(textContent);

// Save to DB
await supabase.from("notes").insert({
  title,
  type: "website",
  summary_markdown: summary,
});
```

---

## ğŸ“ Best Practices

### For Users
1. **Use article URLs** (not landing pages)
2. **Check content is accessible** (no login required)
3. **Prefer text-heavy articles** (not image galleries)
4. **Use standard formats** (blogs, news, Wikipedia)

### For Developers
1. **Handle errors gracefully** (show clear messages)
2. **Validate content length** (ensure quality)
3. **Cache results** (reduce API calls)
4. **Respect rate limits** (don't abuse sites)

---

**âœ¨ Your website processing system is production-ready!**

